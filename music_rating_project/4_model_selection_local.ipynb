{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import string\n",
    "import pickle\n",
    "import tarfile\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_location = 's3://djk-ml-sagemaker/music_lyrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(f'{s3_location}cleaned_lemmatized_unstopped_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>liked</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI can't remember\\nThe words wer...</td>\n",
       "      <td>If You Want It</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remember word form mouth have find bring joy p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI'm not that kind of fool\\nWho ...</td>\n",
       "      <td>Superbike</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kind fool need read room somebody tell fall li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n[Verse 1]\\nPoint me to my chair\\nMake me s...</td>\n",
       "      <td>Peace Out</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>point chair sing awful song bear go hard hard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n[Verse 1]\\nUsed to be the one to cry\\nAnd ...</td>\n",
       "      <td>Devotion</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cry feel emotion need path find strange devoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI'm sinking in my bed\\nWe’re le...</td>\n",
       "      <td>Nighttime Drive</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sink bed be leave town tomorrow memory feel nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics       song_title  \\\n",
       "0  \\n\\n[Verse 1]\\nI can't remember\\nThe words wer...   If You Want It   \n",
       "1  \\n\\n[Verse 1]\\nI'm not that kind of fool\\nWho ...        Superbike   \n",
       "2  \\n\\n[Verse 1]\\nPoint me to my chair\\nMake me s...        Peace Out   \n",
       "3  \\n\\n[Verse 1]\\nUsed to be the one to cry\\nAnd ...         Devotion   \n",
       "4  \\n\\n[Verse 1]\\nI'm sinking in my bed\\nWe’re le...  Nighttime Drive   \n",
       "\n",
       "  artist_name  liked                                     cleaned_lyrics  \n",
       "0     Jay Som    NaN  remember word form mouth have find bring joy p...  \n",
       "1     Jay Som    NaN  kind fool need read room somebody tell fall li...  \n",
       "2     Jay Som    NaN  point chair sing awful song bear go hard hard ...  \n",
       "3     Jay Som    NaN  cry feel emotion need path find strange devoti...  \n",
       "4     Jay Som    NaN  sink bed be leave town tomorrow memory feel nu...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    38071\n",
       "0.0     1656\n",
       "1.0     1639\n",
       "Name: liked, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.liked.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating training/testing datasets, only choosing liked/not liked songs, we will apply the finalized model to unrated music to get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df = final_df[final_df['liked'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3295, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be the pool of songs that will be recommended to us\n",
    "\n",
    "null_df = final_df[final_df['liked'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = non_null_df['cleaned_lyrics'], non_null_df['liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1656\n",
       "1.0    1639\n",
       "Name: liked, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_value_counts = y.value_counts()\n",
    "\n",
    "train_value_counts\n",
    "\n",
    "majority_count = train_value_counts[train_value_counts.index == 0].iloc[0]\n",
    "target_count = train_value_counts[train_value_counts.index == 1].iloc[0]\n",
    "\n",
    "majority_ratio = (majority_count)/(majority_count+target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving training/testing to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(filename, bucket, key):\n",
    "    '''\n",
    "    this function uploads files from local directory to s3\n",
    "    '''\n",
    "    \n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining s3 bucket/key paths\n",
    "\n",
    "bucket_name = 'djk-ml-sagemaker'\n",
    "\n",
    "data_folder = 'music_lyrics'\n",
    "\n",
    "s3_training_location = f's3://{data_folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([y_train, X_train], axis = 1)\n",
    "test = pd.concat([y_test, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index = None)\n",
    "test.to_csv('test.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.to_csv('eligible_song_pool.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3('train.csv', bucket_name, 'music_lyrics/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3('test.csv', bucket_name, 'music_lyrics/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3('eligible_song_pool.csv', bucket_name, 'music_lyrics/eligible_song_pool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "variance_filter = VarianceThreshold(.0005)\n",
    "\n",
    "word_features = ['cleaned_lyrics']\n",
    "\n",
    "steps = [\n",
    "#     ('count_vect', count_vect_no_stops),\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('variance_filter', variance_filter) # removes low variance columns from dataset\n",
    "]\n",
    "\n",
    "word_transformer = Pipeline(steps)\n",
    "\n",
    "# gridsearch params/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2636x397 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 65624 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = word_transformer.named_steps['tfidf_vectorizer'].get_feature_names()\n",
    "filtered_columns = word_transformer.named_steps['variance_filter'].get_support(indices = True)\n",
    "\n",
    "vectorized_X_train = pd.DataFrame(\n",
    "    word_transformer.fit_transform(X_train).toarray(),\n",
    "    columns = [all_columns[i] for i in filtered_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>ah</th>\n",
       "      <th>air</th>\n",
       "      <th>alive</th>\n",
       "      <th>alright</th>\n",
       "      <th>angel</th>\n",
       "      <th>animal</th>\n",
       "      <th>answer</th>\n",
       "      <th>anybody</th>\n",
       "      <th>arm</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worry</th>\n",
       "      <th>wrong</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088649</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   act   ah       air  alive  alright  angel  animal  answer  anybody  arm  \\\n",
       "0  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "1  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "2  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "3  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "4  0.0  0.0  0.260352    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "\n",
       "   ...     world  worry  wrong        ya  yeah  year  yes  yesterday  \\\n",
       "0  ...  0.000000    0.0    0.0  0.052665   0.0   0.0  0.0        0.0   \n",
       "1  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "2  ...  0.056204    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "3  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "4  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "\n",
       "         yo  young  \n",
       "0  0.088649    0.0  \n",
       "1  0.000000    0.0  \n",
       "2  0.000000    0.0  \n",
       "3  0.000000    0.0  \n",
       "4  0.000000    0.0  \n",
       "\n",
       "[5 rows x 397 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2636, 397)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_X_train.head()\n",
    "vectorized_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick model selection. Choosing a handful of classificaiton algorithms with default params to shortlist the promising ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X_train, y_train, list_of_models:list):\n",
    "    '''\n",
    "    this functions takes in a list of classification algorithms with default hyperparam settings\n",
    "    so we can find the ones that are the most promising\n",
    "    '''\n",
    "    \n",
    "    roc_auc_list = []\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for model in list_of_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        feature_importance_dict = {}\n",
    "        \n",
    "        try:\n",
    "            for feature, importance in zip(X_train.columns, model.feature_importances_):\n",
    "                feature_importance_dict[feature] = importance\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        importance_df = pd.DataFrame.from_dict(feature_importance_dict, orient = 'index')\n",
    "        \n",
    "        importances.append(importance_df)\n",
    "        \n",
    "        quick_roc_auc = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            scoring = 'roc_auc',\n",
    "            cv = StratifiedKFold(10)\n",
    "        )\n",
    "        \n",
    "        roc_auc_list.append(np.mean(quick_roc_auc))\n",
    "    \n",
    "    return roc_auc_list, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = [\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state = 42),\n",
    "    SVC(random_state = 42),\n",
    "    RandomForestClassifier(random_state = 42),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(random_state = 42)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_roc_aucs, importances = model_selection(vectorized_X_train, y_train, list_of_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating promising models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.555458191070225,\n",
       " 0.5602956414033106,\n",
       " 0.5624070205440737,\n",
       " 0.5780081443356673,\n",
       " 0.5256359218262046,\n",
       " 0.5659890646616331]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection_roc_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "random_forest_top_features = importances[3][0].sort_values(ascending = False)[:50]\n",
    "xgb_top_features = importances[5][0].sort_values(ascending = False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest and xgboost are coming out ahead, looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know     0.018697\n",
       "come     0.012762\n",
       "love     0.012172\n",
       "want     0.011419\n",
       "yeah     0.011078\n",
       "like     0.010559\n",
       "right    0.010087\n",
       "oh       0.009537\n",
       "time     0.009240\n",
       "feel     0.008529\n",
       "let      0.008474\n",
       "look     0.008187\n",
       "good     0.008077\n",
       "think    0.008065\n",
       "hear     0.007639\n",
       "tell     0.007439\n",
       "say      0.007327\n",
       "away     0.007022\n",
       "leave    0.006989\n",
       "way      0.006974\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "town      0.016324\n",
       "money     0.012873\n",
       "yes       0.012717\n",
       "hear      0.012581\n",
       "right     0.011584\n",
       "arm       0.011488\n",
       "damn      0.011459\n",
       "kick      0.011148\n",
       "big       0.010778\n",
       "hate      0.010303\n",
       "scream    0.010198\n",
       "room      0.010052\n",
       "run       0.010035\n",
       "walk      0.009701\n",
       "whoa      0.009527\n",
       "hey       0.009510\n",
       "rock      0.009500\n",
       "music     0.009449\n",
       "away      0.009367\n",
       "soon      0.009252\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_top_features[:20]\n",
    "xgb_top_features[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'away',\n",
       " 'boy',\n",
       " 'come',\n",
       " 'feeling',\n",
       " 'good',\n",
       " 'hear',\n",
       " 'hey',\n",
       " 'home',\n",
       " 'leave',\n",
       " 'let',\n",
       " 'make',\n",
       " 'man',\n",
       " 'right',\n",
       " 'say',\n",
       " 'true',\n",
       " 'walk',\n",
       " 'yeah'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature_overlap = set(random_forest_top_features.index) & set(xgb_top_features.index)\n",
    "\n",
    "top_feature_overlap\n",
    "\n",
    "len(top_feature_overlap)/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is a 32% overlap in the top 50 features for random forest and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X_train.to_csv('vectorized_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking the top 2 performing models (random forest and xgboost) and optimizing hyperparameters (randomizedsearch) for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(model_hyperparam_dict, X_train, y_train):\n",
    "    '''\n",
    "    this function finds the best model.\n",
    "    takes in a dictionary with multiple algorithms and hyperparameters\n",
    "    returns the best model based on roc_auc cross val scores\n",
    "    also leaves a file with best params\n",
    "    '''\n",
    "    \n",
    "    keys = [k for k, v in model_hyperparam_dict.items()]\n",
    "    \n",
    "    cv_results = []\n",
    "    best_scores = []\n",
    "    best_estimators = []\n",
    "    classification_reports = []\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        model = model_hyperparam_dict[key]['model']\n",
    "        hyperparams = model_hyperparam_dict[key]['hyperparams']\n",
    "        \n",
    "        print(f'randomsearching {key}...')\n",
    "        print(f'using hyperparams: \\n{hyperparams}')\n",
    "        \n",
    "        grid = RandomizedSearchCV(\n",
    "            estimator = model,\n",
    "            param_distributions = hyperparams,\n",
    "            n_iter = 100,\n",
    "            scoring = 'roc_auc',\n",
    "            cv = StratifiedKFold(10),\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        cv_results.append(grid.cv_results_)\n",
    "        best_scores.append(grid.best_score_)\n",
    "        best_estimators.append(grid.best_estimator_)\n",
    "        \n",
    "    return cv_results, best_scores, best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparams = {\n",
    "    'bootstrap':[True, False],\n",
    "    'max_depth':[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features':['auto', 'sqrt'],\n",
    "    'min_samples_leaf':[1, 2, 4],\n",
    "    'min_samples_split':[2, 5, 10],\n",
    "    'n_estimators':[100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "xgb_hyperparams = {\n",
    "    'max_depth':range(1, 15),\n",
    "    'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators':[100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "    'gamma':[0, .01, .1, 1],\n",
    "    'min_child_weight':[1, 3, 5, 7],\n",
    "    'colsample_bytree':[.3, .4, .5, .6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparam_dict = {\n",
    "    'random_forest' : {\n",
    "        'model': RandomForestClassifier(random_state = 42),\n",
    "        'hyperparams' : random_forest_hyperparams\n",
    "    },\n",
    "    \n",
    "    'xgboost' : {\n",
    "        'model':XGBClassifier(random_state = 42),\n",
    "        'hyperparams' : xgb_hyperparams\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomsearching random_forest...\n",
      "using hyperparams: \n",
      "{'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/music_rating/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  raise TypeError(\"shuffle must be True or False;\"\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed: 173.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomsearching xgboost...\n",
      "using hyperparams: \n",
      "{'max_depth': range(1, 15), 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'gamma': [0, 0.01, 0.1, 1], 'min_child_weight': [1, 3, 5, 7], 'colsample_bytree': [0.3, 0.4, 0.5, 0.6]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/music_rating/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  raise TypeError(\"shuffle must be True or False;\"\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed: 382.1min finished\n"
     ]
    }
   ],
   "source": [
    "cv_results, best_scores, best_estimators = find_best_model(model_hyperparam_dict, vectorized_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5920927138441018, 0.5845631744848526]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=80, max_features='sqrt',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=600,\n",
       "                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                        warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.3, gamma=0.1,\n",
       "               learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "               min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "               nthread=None, objective='binary:logistic', random_state=42,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "               silent=None, subsample=1, verbosity=1)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### appears to perform better than chance, since the training dataset contains about 50% labeled as liked and unliked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building a pipeline for both using the optimized hyperparams to evaluate on the test set, since our roc_auc crossval score is close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = best_estimators[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = best_estimators[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pipeline_steps = [\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer), # term frequency document infrequency word vectorizer\n",
    "    ('variance_filter', variance_filter), # removes low variance columns from dataset\n",
    "    ('classifier', best_random_forest)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_pipeline_steps = [\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer), # term frequency document infrequency word vectorizer\n",
    "    ('variance_filter', variance_filter), # removes low variance columns from dataset\n",
    "    ('classifier', best_xgb)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_random_forest_model = Pipeline(second_pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False...\n",
       "                 RandomForestClassifier(bootstrap=False, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=80, max_features='sqrt',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=2, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=600, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False...\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=0.3, gamma=0.1,\n",
       "                               learning_rate=0.01, max_delta_step=0,\n",
       "                               max_depth=12, min_child_weight=1, missing=None,\n",
       "                               n_estimators=200, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=42,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_xgb_model = Pipeline(third_pipeline_steps)\n",
    "final_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_preds = final_random_forest_model.predict(X_test)\n",
    "random_forest_probas = final_random_forest_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = final_xgb_model.predict(X_test)\n",
    "xgb_probas = final_xgb_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating both models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5475722692641172"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, random_forest_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.59      0.58       342\n",
      "         1.0       0.53      0.50      0.52       317\n",
      "\n",
      "    accuracy                           0.55       659\n",
      "   macro avg       0.55      0.55      0.55       659\n",
      "weighted avg       0.55      0.55      0.55       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, random_forest_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0527, 0.0968]      2\n",
       "(0.0968, 0.139]       4\n",
       "(0.139, 0.181]        2\n",
       "(0.181, 0.224]        3\n",
       "(0.224, 0.266]        1\n",
       "(0.266, 0.308]        9\n",
       "(0.308, 0.35]        30\n",
       "(0.35, 0.392]        42\n",
       "(0.392, 0.435]       79\n",
       "(0.435, 0.477]      119\n",
       "(0.477, 0.519]      116\n",
       "(0.519, 0.561]       66\n",
       "(0.561, 0.604]       50\n",
       "(0.604, 0.646]       28\n",
       "(0.646, 0.688]       47\n",
       "(0.688, 0.73]        28\n",
       "(0.73, 0.773]        12\n",
       "(0.773, 0.815]        9\n",
       "(0.815, 0.857]        5\n",
       "(0.857, 0.899]        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([l[1] for l in random_forest_probas]).value_counts(bins = 20).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5701523788440608"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.54      0.57       342\n",
      "         1.0       0.55      0.60      0.57       317\n",
      "\n",
      "    accuracy                           0.57       659\n",
      "   macro avg       0.57      0.57      0.57       659\n",
      "weighted avg       0.57      0.57      0.57       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, xgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.231, 0.255]     4\n",
       "(0.255, 0.278]     0\n",
       "(0.278, 0.3]       3\n",
       "(0.3, 0.323]       3\n",
       "(0.323, 0.346]     7\n",
       "(0.346, 0.368]    10\n",
       "(0.368, 0.391]    33\n",
       "(0.391, 0.414]    31\n",
       "(0.414, 0.437]    47\n",
       "(0.437, 0.459]    57\n",
       "(0.459, 0.482]    59\n",
       "(0.482, 0.505]    71\n",
       "(0.505, 0.527]    73\n",
       "(0.527, 0.55]     63\n",
       "(0.55, 0.573]     58\n",
       "(0.573, 0.596]    88\n",
       "(0.596, 0.618]    28\n",
       "(0.618, 0.641]    13\n",
       "(0.641, 0.664]     4\n",
       "(0.664, 0.686]     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([l[1] for l in xgb_probas]).value_counts(bins = 20).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looks like we're losing some % on the eval set for both models, but the xgboost is performing better than randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving both models as backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_xgb_model, open('xgb_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"xgb_model.tar.gz\", \"w:gz\")\n",
    "tar.add('xgb_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_random_forest_model, open('random_forest.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"random_forest.tar.gz\", \"w:gz\")\n",
    "tar.add('random_forest.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_music_rating_project",
   "language": "python",
   "name": "conda_music_rating_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
