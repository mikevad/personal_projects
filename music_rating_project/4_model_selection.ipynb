{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pandas_profiling as pp\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import tarfile\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_location = 's3://djk-ml-sagemaker/music_lyrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(f'{s3_location}cleaned_lemmatized_unstopped_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    38071\n",
       "0.0     1656\n",
       "1.0     1639\n",
       "Name: liked, dtype: int64"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.liked.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_df = final_df[final_df['liked'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3295, 5)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = final_df[final_df['liked'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = non_null_df['cleaned_lyrics'], non_null_df['liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1656\n",
       "1.0    1639\n",
       "Name: liked, dtype: int64"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_value_counts = y.value_counts()\n",
    "\n",
    "train_value_counts\n",
    "\n",
    "majority_count = train_value_counts[train_value_counts.index == 0].iloc[0]\n",
    "target_count = train_value_counts[train_value_counts.index == 1].iloc[0]\n",
    "\n",
    "majority_ratio = (majority_count)/(majority_count+target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "variance_filter = VarianceThreshold(.0005)\n",
    "\n",
    "word_features = ['cleaned_lyrics']\n",
    "\n",
    "steps = [\n",
    "#     ('count_vect', count_vect_no_stops),\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('variance_filter', variance_filter) # removes low variance columns from dataset\n",
    "]\n",
    "\n",
    "word_transformer = Pipeline(steps)\n",
    "\n",
    "# gridsearch params/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2636x397 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 65624 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = word_transformer.named_steps['tfidf_vectorizer'].get_feature_names()\n",
    "filtered_columns = word_transformer.named_steps['variance_filter'].get_support(indices = True)\n",
    "\n",
    "vectorized_X_train = pd.DataFrame(\n",
    "    word_transformer.fit_transform(X_train).toarray(),\n",
    "    columns = [all_columns[i] for i in filtered_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>ah</th>\n",
       "      <th>air</th>\n",
       "      <th>alive</th>\n",
       "      <th>alright</th>\n",
       "      <th>angel</th>\n",
       "      <th>animal</th>\n",
       "      <th>answer</th>\n",
       "      <th>anybody</th>\n",
       "      <th>arm</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worry</th>\n",
       "      <th>wrong</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088649</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   act   ah       air  alive  alright  angel  animal  answer  anybody  arm  \\\n",
       "0  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "1  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "2  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "3  0.0  0.0  0.000000    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "4  0.0  0.0  0.260352    0.0      0.0    0.0     0.0     0.0      0.0  0.0   \n",
       "\n",
       "   ...     world  worry  wrong        ya  yeah  year  yes  yesterday  \\\n",
       "0  ...  0.000000    0.0    0.0  0.052665   0.0   0.0  0.0        0.0   \n",
       "1  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "2  ...  0.056204    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "3  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "4  ...  0.000000    0.0    0.0  0.000000   0.0   0.0  0.0        0.0   \n",
       "\n",
       "         yo  young  \n",
       "0  0.088649    0.0  \n",
       "1  0.000000    0.0  \n",
       "2  0.000000    0.0  \n",
       "3  0.000000    0.0  \n",
       "4  0.000000    0.0  \n",
       "\n",
       "[5 rows x 397 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2636, 397)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_X_train.head()\n",
    "vectorized_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X_train, y_train, list_of_models:list):\n",
    "    '''\n",
    "    this functions takes in a list of classification algorithms with default hyperparam settings\n",
    "    so we can find the ones that are the most promising\n",
    "    '''\n",
    "    \n",
    "    roc_auc_list = []\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for model in list_of_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        feature_importance_dict = {}\n",
    "        \n",
    "        try:\n",
    "            for feature, importance in zip(X_train.columns, model.feature_importances_):\n",
    "                feature_importance_dict[feature] = importance\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        importance_df = pd.DataFrame.from_dict(feature_importance_dict, orient = 'index')\n",
    "        \n",
    "        importances.append(importance_df)\n",
    "        \n",
    "        quick_roc_auc = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            scoring = 'roc_auc',\n",
    "            cv = StratifiedKFold(10)\n",
    "        )\n",
    "        \n",
    "        roc_auc_list.append(np.mean(quick_roc_auc))\n",
    "    \n",
    "    return roc_auc_list, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = [\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state = 42),\n",
    "    SVC(random_state = 42),\n",
    "    RandomForestClassifier(random_state = 42),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(random_state = 42)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_selection_roc_aucs, importances = model_selection(vectorized_X_train, y_train, list_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5550139538605836,\n",
       " 0.560144619765246,\n",
       " 0.5564328412653087,\n",
       " 0.5729444748089285,\n",
       " 0.5283464785723588,\n",
       " 0.566533049431667]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Empty DataFrame\n",
       " Columns: []\n",
       " Index: [], Empty DataFrame\n",
       " Columns: []\n",
       " Index: [], Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],                      0\n",
       " act           0.001949\n",
       " ah            0.003459\n",
       " air           0.002539\n",
       " alive         0.002364\n",
       " alright       0.002611\n",
       " angel         0.000740\n",
       " animal        0.000413\n",
       " answer        0.001489\n",
       " anybody       0.000367\n",
       " arm           0.001253\n",
       " ask           0.001910\n",
       " away          0.007598\n",
       " babe          0.000399\n",
       " baby          0.008308\n",
       " bad           0.004834\n",
       " bang          0.000748\n",
       " bass          0.001458\n",
       " bear          0.002915\n",
       " beat          0.002257\n",
       " beautiful     0.000745\n",
       " begin         0.000918\n",
       " believe       0.003660\n",
       " belong        0.001347\n",
       " better        0.001489\n",
       " big           0.002373\n",
       " bit           0.000130\n",
       " bitch         0.002005\n",
       " black         0.002309\n",
       " blind         0.001557\n",
       " blood         0.002692\n",
       " blow          0.004048\n",
       " blue          0.001678\n",
       " body          0.002070\n",
       " boy           0.005189\n",
       " break         0.002693\n",
       " breathe       0.002632\n",
       " bring         0.001530\n",
       " broken        0.000719\n",
       " brother       0.000905\n",
       " build         0.001173\n",
       " burn          0.003485\n",
       " bury          0.000914\n",
       " car           0.001266\n",
       " care          0.002402\n",
       " carry         0.000331\n",
       " casbah        0.000000\n",
       " catch         0.002115\n",
       " cause         0.005723\n",
       " chance        0.001512\n",
       " change        0.003085\n",
       " check         0.002522\n",
       " child         0.001656\n",
       " choose        0.000644\n",
       " chorus        0.001033\n",
       " city          0.002166\n",
       " clap          0.000071\n",
       " close         0.003654\n",
       " closer        0.000473\n",
       " cold          0.001862\n",
       " color         0.000979\n",
       " come          0.011437\n",
       " contain       0.001336\n",
       " control       0.002577\n",
       " cool          0.001952\n",
       " crazy         0.002314\n",
       " crew          0.000565\n",
       " cruel         0.000002\n",
       " crush         0.000084\n",
       " cut           0.002665\n",
       " da            0.002046\n",
       " daddy         0.000968\n",
       " damn          0.002769\n",
       " dance         0.004947\n",
       " dancing       0.000136\n",
       " dark          0.003830\n",
       " darling       0.001524\n",
       " day           0.008283\n",
       " dead          0.003919\n",
       " dear          0.000919\n",
       " dee           0.000000\n",
       " deep          0.001743\n",
       " die           0.003898\n",
       " dig           0.000404\n",
       " dog           0.000953\n",
       " door          0.001739\n",
       " dream         0.005719\n",
       " drive         0.001990\n",
       " drop          0.003310\n",
       " drum          0.000701\n",
       " dust          0.000342\n",
       " earth         0.001973\n",
       " easy          0.002277\n",
       " end           0.003824\n",
       " escape        0.000053\n",
       " eu            0.001657\n",
       " everybody     0.003568\n",
       " explore       0.000374\n",
       " eye           0.006341\n",
       " face          0.006098\n",
       " fade          0.001238\n",
       " fall          0.004277\n",
       " far           0.002137\n",
       " fast          0.002415\n",
       " fear          0.002428\n",
       " feel          0.009281\n",
       " feeling       0.004576\n",
       " fight         0.002330\n",
       " figure        0.000768\n",
       " finally       0.000568\n",
       " fine          0.002541\n",
       " floor         0.000928\n",
       " flower        0.000019\n",
       " fly           0.001711\n",
       " follow        0.000234\n",
       " fool          0.001456\n",
       " foot          0.002612\n",
       " forever       0.001382\n",
       " forget        0.000742\n",
       " freak         0.002744\n",
       " free          0.002069\n",
       " fresh         0.000541\n",
       " friend        0.004193\n",
       " funk          0.000380\n",
       " game          0.002984\n",
       " girl          0.006175\n",
       " god           0.001187\n",
       " gold          0.000615\n",
       " golden        0.000491\n",
       " gon           0.000873\n",
       " good          0.005573\n",
       " got           0.000981\n",
       " great         0.002483\n",
       " grey          0.000274\n",
       " groove        0.000298\n",
       " ground        0.004384\n",
       " grow          0.003237\n",
       " guess         0.001868\n",
       " gun           0.001404\n",
       " ha            0.000331\n",
       " hallelujah    0.000303\n",
       " hand          0.006713\n",
       " happen        0.003102\n",
       " happy         0.003710\n",
       " hard          0.002744\n",
       " harmony       0.000233\n",
       " hate          0.001311\n",
       " head          0.004448\n",
       " hear          0.008971\n",
       " heart         0.005182\n",
       " heaven        0.000688\n",
       " help          0.003220\n",
       " hero          0.000079\n",
       " hey           0.006971\n",
       " hi            0.000122\n",
       " hide          0.002969\n",
       " high          0.003127\n",
       " ho            0.000678\n",
       " hold          0.004809\n",
       " home          0.005834\n",
       " honey         0.000878\n",
       " hook          0.001185\n",
       " hope          0.003741\n",
       " hot           0.001923\n",
       " hour          0.000966\n",
       " house         0.001727\n",
       " huh           0.002000\n",
       " hurt          0.001978\n",
       " insane        0.000315\n",
       " inside        0.002863\n",
       " instrumental  0.002291\n",
       " intention     0.001156\n",
       " jazz          0.000000\n",
       " jump          0.002564\n",
       " kick          0.002396\n",
       " kill          0.001076\n",
       " kind          0.001506\n",
       " kiss          0.001415\n",
       " know          0.018959\n",
       " la            0.003164\n",
       " lady          0.002657\n",
       " late          0.002145\n",
       " laugh         0.001473\n",
       " lean          0.000487\n",
       " learn         0.002438\n",
       " leave         0.005861\n",
       " let           0.008537\n",
       " lie           0.003044\n",
       " life          0.004327\n",
       " light         0.006707\n",
       " like          0.009961\n",
       " line          0.001839\n",
       " listen        0.002898\n",
       " little        0.003970\n",
       " live          0.006054\n",
       " lonely        0.002184\n",
       " long          0.006427\n",
       " longer        0.001186\n",
       " look          0.007690\n",
       " lord          0.001074\n",
       " lose          0.007978\n",
       " lot           0.001886\n",
       " love          0.011587\n",
       " lover         0.000715\n",
       " low           0.001802\n",
       " lyric         0.002427\n",
       " machine       0.000976\n",
       " magic         0.000274\n",
       " make          0.003714\n",
       " man           0.006370\n",
       " matter        0.001790\n",
       " maybe         0.003319\n",
       " mean          0.003226\n",
       " meet          0.003168\n",
       " memory        0.002428\n",
       " mi            0.002577\n",
       " mile          0.001055\n",
       " mind          0.005726\n",
       " miracle       0.000377\n",
       " miss          0.001953\n",
       " moment        0.002180\n",
       " mon           0.001297\n",
       " money         0.003193\n",
       " morning       0.001749\n",
       " mountain      0.000079\n",
       " music         0.003092\n",
       " na            0.001300\n",
       " need          0.008093\n",
       " new           0.004772\n",
       " nice          0.002250\n",
       " night         0.006460\n",
       " number        0.000513\n",
       " oh            0.011412\n",
       " old           0.004267\n",
       " open          0.003808\n",
       " outside       0.001974\n",
       " owe           0.000175\n",
       " pain          0.000887\n",
       " party         0.000595\n",
       " pass          0.002186\n",
       " past          0.001326\n",
       " peace         0.001004\n",
       " people        0.002254\n",
       " perfect       0.001243\n",
       " place         0.002517\n",
       " play          0.003244\n",
       " pleasure      0.000637\n",
       " point         0.000911\n",
       " pop           0.001778\n",
       " pour          0.001475\n",
       " power         0.000764\n",
       " praise        0.000317\n",
       " pressure      0.000467\n",
       " pretend       0.002207\n",
       " pretty        0.001109\n",
       " pride         0.001238\n",
       " problem       0.002299\n",
       " promise       0.000878\n",
       " pull          0.001397\n",
       " push          0.002981\n",
       " radio         0.000000\n",
       " rain          0.002962\n",
       " rainbow       0.000688\n",
       " raindrop      0.000488\n",
       " reach         0.001572\n",
       " ready         0.001600\n",
       " real          0.003729\n",
       " reason        0.001861\n",
       " release       0.000003\n",
       " remember      0.005032\n",
       " remind        0.000736\n",
       " repeat        0.000751\n",
       " rest          0.001024\n",
       " ride          0.001941\n",
       " right         0.008229\n",
       " ring          0.000479\n",
       " river         0.000741\n",
       " road          0.002480\n",
       " robot         0.000599\n",
       " rock          0.003725\n",
       " roll          0.001278\n",
       " romance       0.000723\n",
       " room          0.001807\n",
       " round         0.002297\n",
       " run           0.002911\n",
       " sad           0.002166\n",
       " satisfaction  0.001511\n",
       " save          0.001059\n",
       " say           0.007851\n",
       " scream        0.002013\n",
       " se            0.003856\n",
       " search        0.000615\n",
       " second        0.000855\n",
       " seed          0.000154\n",
       " set           0.003265\n",
       " sex           0.001415\n",
       " sexy          0.000643\n",
       " shadow        0.001295\n",
       " shake         0.001300\n",
       " shine         0.001986\n",
       " shoot         0.001171\n",
       " shout         0.000976\n",
       " sight         0.000915\n",
       " sing          0.002232\n",
       " singe         0.000519\n",
       " sit           0.000463\n",
       " sky           0.001689\n",
       " sleep         0.004180\n",
       " slide         0.000451\n",
       " slip          0.000630\n",
       " slow          0.001370\n",
       " smile         0.001940\n",
       " sock          0.000234\n",
       " somebody      0.002680\n",
       " song          0.002151\n",
       " soon          0.001869\n",
       " soul          0.002130\n",
       " sound         0.002896\n",
       " soup          0.000125\n",
       " special       0.000805\n",
       " spend         0.001159\n",
       " spooky        0.000000\n",
       " stand         0.003962\n",
       " star          0.003717\n",
       " starlight     0.000000\n",
       " start         0.005881\n",
       " stay          0.005191\n",
       " step          0.002662\n",
       " stop          0.002417\n",
       " strange       0.001943\n",
       " street        0.002344\n",
       " strong        0.001552\n",
       " subway        0.000064\n",
       " sugar         0.001844\n",
       " summer        0.002386\n",
       " sun           0.002084\n",
       " sunshine      0.000484\n",
       " sure          0.001455\n",
       " sweat         0.001039\n",
       " sweet         0.002304\n",
       " talk          0.004230\n",
       " tear          0.002535\n",
       " tell          0.007649\n",
       " thank         0.000527\n",
       " thing         0.008474\n",
       " think         0.007795\n",
       " thought       0.001579\n",
       " throw         0.006247\n",
       " til           0.004133\n",
       " till          0.001524\n",
       " time          0.008147\n",
       " tired         0.000984\n",
       " today         0.002303\n",
       " tomorrow      0.001293\n",
       " tonight       0.005176\n",
       " touch         0.002449\n",
       " town          0.002235\n",
       " trouble       0.000574\n",
       " true          0.005600\n",
       " trust         0.001189\n",
       " try           0.005667\n",
       " tu            0.000677\n",
       " turn          0.005258\n",
       " understand    0.002750\n",
       " wait          0.002616\n",
       " wake          0.001400\n",
       " walk          0.005356\n",
       " wall          0.002568\n",
       " want          0.012732\n",
       " warm          0.000168\n",
       " waste         0.001813\n",
       " watch         0.003725\n",
       " water         0.002412\n",
       " wave          0.002088\n",
       " way           0.007109\n",
       " weak          0.000194\n",
       " white         0.002375\n",
       " whoa          0.005137\n",
       " wide          0.000116\n",
       " wild          0.001500\n",
       " win           0.000405\n",
       " wish          0.001915\n",
       " witness       0.000150\n",
       " woman         0.001026\n",
       " wonder        0.001358\n",
       " woo           0.001065\n",
       " word          0.002917\n",
       " work          0.004635\n",
       " world         0.005115\n",
       " worry         0.000494\n",
       " wrong         0.002190\n",
       " ya            0.003650\n",
       " yeah          0.012899\n",
       " year          0.002361\n",
       " yes           0.004724\n",
       " yesterday     0.000432\n",
       " yo            0.002560\n",
       " young         0.000955, Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],                      0\n",
       " act           0.000000\n",
       " ah            0.000150\n",
       " air           0.000000\n",
       " alive         0.000000\n",
       " alright       0.000000\n",
       " angel         0.000000\n",
       " animal        0.000000\n",
       " answer        0.000000\n",
       " anybody       0.000000\n",
       " arm           0.011488\n",
       " ask           0.000000\n",
       " away          0.009367\n",
       " babe          0.004837\n",
       " baby          0.000000\n",
       " bad           0.006087\n",
       " bang          0.000000\n",
       " bass          0.000000\n",
       " bear          0.005853\n",
       " beat          0.006985\n",
       " beautiful     0.000000\n",
       " begin         0.000000\n",
       " believe       0.000000\n",
       " belong        0.005094\n",
       " better        0.000000\n",
       " big           0.010778\n",
       " bit           0.000000\n",
       " bitch         0.000000\n",
       " black         0.007675\n",
       " blind         0.000000\n",
       " blood         0.005842\n",
       " blow          0.000000\n",
       " blue          0.000000\n",
       " body          0.004679\n",
       " boy           0.009044\n",
       " break         0.004135\n",
       " breathe       0.000000\n",
       " bring         0.005105\n",
       " broken        0.000000\n",
       " brother       0.000000\n",
       " build         0.004387\n",
       " burn          0.000000\n",
       " bury          0.000000\n",
       " car           0.000000\n",
       " care          0.000000\n",
       " carry         0.000000\n",
       " casbah        0.000000\n",
       " catch         0.000000\n",
       " cause         0.003223\n",
       " chance        0.000000\n",
       " change        0.005104\n",
       " check         0.000000\n",
       " child         0.000000\n",
       " choose        0.000000\n",
       " chorus        0.004900\n",
       " city          0.000000\n",
       " clap          0.000000\n",
       " close         0.000000\n",
       " closer        0.000000\n",
       " cold          0.000000\n",
       " color         0.000000\n",
       " come          0.008330\n",
       " contain       0.000000\n",
       " control       0.004880\n",
       " cool          0.000000\n",
       " crazy         0.000000\n",
       " crew          0.000000\n",
       " cruel         0.007037\n",
       " crush         0.000000\n",
       " cut           0.000000\n",
       " da            0.004309\n",
       " daddy         0.005510\n",
       " damn          0.011459\n",
       " dance         0.000000\n",
       " dancing       0.000000\n",
       " dark          0.000000\n",
       " darling       0.004873\n",
       " day           0.003604\n",
       " dead          0.004728\n",
       " dear          0.005559\n",
       " dee           0.000000\n",
       " deep          0.000000\n",
       " die           0.007343\n",
       " dig           0.000000\n",
       " dog           0.000000\n",
       " door          0.000000\n",
       " dream         0.005848\n",
       " drive         0.000000\n",
       " drop          0.006549\n",
       " drum          0.000000\n",
       " dust          0.000000\n",
       " earth         0.000000\n",
       " easy          0.000000\n",
       " end           0.006598\n",
       " escape        0.000000\n",
       " eu            0.000000\n",
       " everybody     0.000000\n",
       " explore       0.000000\n",
       " eye           0.002994\n",
       " face          0.004706\n",
       " fade          0.000000\n",
       " fall          0.000000\n",
       " far           0.000000\n",
       " fast          0.007097\n",
       " fear          0.000000\n",
       " feel          0.004833\n",
       " feeling       0.009120\n",
       " fight         0.000000\n",
       " figure        0.000000\n",
       " finally       0.000000\n",
       " fine          0.003603\n",
       " floor         0.000000\n",
       " flower        0.006192\n",
       " fly           0.000000\n",
       " follow        0.000000\n",
       " fool          0.000000\n",
       " foot          0.005925\n",
       " forever       0.000000\n",
       " forget        0.003344\n",
       " freak         0.006741\n",
       " free          0.008046\n",
       " fresh         0.000000\n",
       " friend        0.000000\n",
       " funk          0.000000\n",
       " game          0.000000\n",
       " girl          0.005919\n",
       " god           0.000000\n",
       " gold          0.000000\n",
       " golden        0.000000\n",
       " gon           0.005337\n",
       " good          0.007717\n",
       " got           0.000000\n",
       " great         0.000000\n",
       " grey          0.000000\n",
       " groove        0.000000\n",
       " ground        0.000000\n",
       " grow          0.000000\n",
       " guess         0.000000\n",
       " gun           0.005389\n",
       " ha            0.000000\n",
       " hallelujah    0.000000\n",
       " hand          0.006771\n",
       " happen        0.000000\n",
       " happy         0.008803\n",
       " hard          0.000000\n",
       " harmony       0.000000\n",
       " hate          0.010303\n",
       " head          0.005585\n",
       " hear          0.012581\n",
       " heart         0.004178\n",
       " heaven        0.000000\n",
       " help          0.000000\n",
       " hero          0.000000\n",
       " hey           0.009510\n",
       " hi            0.000000\n",
       " hide          0.009176\n",
       " high          0.000000\n",
       " ho            0.000000\n",
       " hold          0.000000\n",
       " home          0.009207\n",
       " honey         0.000000\n",
       " hook          0.000000\n",
       " hope          0.000000\n",
       " hot           0.006891\n",
       " hour          0.000000\n",
       " house         0.005944\n",
       " huh           0.006098\n",
       " hurt          0.006965\n",
       " insane        0.000000\n",
       " inside        0.000000\n",
       " instrumental  0.000000\n",
       " intention     0.000000\n",
       " jazz          0.000000\n",
       " jump          0.000000\n",
       " kick          0.011148\n",
       " kill          0.000000\n",
       " kind          0.000000\n",
       " kiss          0.000000\n",
       " know          0.006261\n",
       " la            0.000000\n",
       " lady          0.000000\n",
       " late          0.008288\n",
       " laugh         0.006862\n",
       " lean          0.000000\n",
       " learn         0.000000\n",
       " leave         0.008306\n",
       " let           0.008269\n",
       " lie           0.007615\n",
       " life          0.000000\n",
       " light         0.004909\n",
       " like          0.003383\n",
       " line          0.000000\n",
       " listen        0.000000\n",
       " little        0.000000\n",
       " live          0.006392\n",
       " lonely        0.006804\n",
       " long          0.000000\n",
       " longer        0.007560\n",
       " look          0.006362\n",
       " lord          0.004502\n",
       " lose          0.005290\n",
       " lot           0.000000\n",
       " love          0.004276\n",
       " lover         0.005455\n",
       " low           0.000000\n",
       " lyric         0.000000\n",
       " machine       0.000000\n",
       " magic         0.000000\n",
       " make          0.007322\n",
       " man           0.008242\n",
       " matter        0.005274\n",
       " maybe         0.006507\n",
       " mean          0.004693\n",
       " meet          0.006793\n",
       " memory        0.006065\n",
       " mi            0.000000\n",
       " mile          0.000000\n",
       " mind          0.006367\n",
       " miracle       0.000000\n",
       " miss          0.000000\n",
       " moment        0.004813\n",
       " mon           0.000000\n",
       " money         0.012873\n",
       " morning       0.000000\n",
       " mountain      0.000000\n",
       " music         0.009449\n",
       " na            0.000000\n",
       " need          0.001776\n",
       " new           0.000000\n",
       " nice          0.008874\n",
       " night         0.005294\n",
       " number        0.000000\n",
       " oh            0.000000\n",
       " old           0.008057\n",
       " open          0.006073\n",
       " outside       0.000000\n",
       " owe           0.000000\n",
       " pain          0.000000\n",
       " party         0.000000\n",
       " pass          0.000000\n",
       " past          0.000000\n",
       " peace         0.000000\n",
       " people        0.000000\n",
       " perfect       0.005968\n",
       " place         0.000000\n",
       " play          0.005140\n",
       " pleasure      0.000000\n",
       " point         0.000000\n",
       " pop           0.004175\n",
       " pour          0.004676\n",
       " power         0.000000\n",
       " praise        0.000000\n",
       " pressure      0.000000\n",
       " pretend       0.000000\n",
       " pretty        0.000000\n",
       " pride         0.000000\n",
       " problem       0.000000\n",
       " promise       0.000000\n",
       " pull          0.000000\n",
       " push          0.000000\n",
       " radio         0.000000\n",
       " rain          0.000000\n",
       " rainbow       0.000000\n",
       " raindrop      0.000000\n",
       " reach         0.000000\n",
       " ready         0.005650\n",
       " real          0.000000\n",
       " reason        0.000000\n",
       " release       0.000000\n",
       " remember      0.008881\n",
       " remind        0.000000\n",
       " repeat        0.000000\n",
       " rest          0.000000\n",
       " ride          0.008982\n",
       " right         0.011584\n",
       " ring          0.000000\n",
       " river         0.000000\n",
       " road          0.000000\n",
       " robot         0.000000\n",
       " rock          0.009500\n",
       " roll          0.000000\n",
       " romance       0.000000\n",
       " room          0.010052\n",
       " round         0.000000\n",
       " run           0.010035\n",
       " sad           0.000000\n",
       " satisfaction  0.000000\n",
       " save          0.000000\n",
       " say           0.008121\n",
       " scream        0.010198\n",
       " se            0.000000\n",
       " search        0.000000\n",
       " second        0.000000\n",
       " seed          0.000000\n",
       " set           0.007069\n",
       " sex           0.000000\n",
       " sexy          0.000000\n",
       " shadow        0.007206\n",
       " shake         0.000000\n",
       " shine         0.000000\n",
       " shoot         0.000000\n",
       " shout         0.000000\n",
       " sight         0.000000\n",
       " sing          0.000000\n",
       " singe         0.000000\n",
       " sit           0.007651\n",
       " sky           0.007490\n",
       " sleep         0.002575\n",
       " slide         0.006140\n",
       " slip          0.000000\n",
       " slow          0.000000\n",
       " smile         0.006713\n",
       " sock          0.000000\n",
       " somebody      0.006351\n",
       " song          0.000000\n",
       " soon          0.009252\n",
       " soul          0.005621\n",
       " sound         0.000000\n",
       " soup          0.000000\n",
       " special       0.000000\n",
       " spend         0.000000\n",
       " spooky        0.000000\n",
       " stand         0.000000\n",
       " star          0.000000\n",
       " starlight     0.000000\n",
       " start         0.002664\n",
       " stay          0.007062\n",
       " step          0.000000\n",
       " stop          0.000000\n",
       " strange       0.006814\n",
       " street        0.000000\n",
       " strong        0.000000\n",
       " subway        0.000000\n",
       " sugar         0.000000\n",
       " summer        0.000000\n",
       " sun           0.000000\n",
       " sunshine      0.000000\n",
       " sure          0.000000\n",
       " sweat         0.000000\n",
       " sweet         0.009169\n",
       " talk          0.006639\n",
       " tear          0.000000\n",
       " tell          0.002146\n",
       " thank         0.000000\n",
       " thing         0.005574\n",
       " think         0.006132\n",
       " thought       0.007083\n",
       " throw         0.006963\n",
       " til           0.008636\n",
       " till          0.000000\n",
       " time          0.003969\n",
       " tired         0.000000\n",
       " today         0.004836\n",
       " tomorrow      0.006273\n",
       " tonight       0.004818\n",
       " touch         0.000000\n",
       " town          0.016324\n",
       " trouble       0.000000\n",
       " true          0.007417\n",
       " trust         0.004991\n",
       " try           0.001134\n",
       " tu            0.000000\n",
       " turn          0.000000\n",
       " understand    0.000000\n",
       " wait          0.002127\n",
       " wake          0.007218\n",
       " walk          0.009701\n",
       " wall          0.000000\n",
       " want          0.006905\n",
       " warm          0.000000\n",
       " waste         0.000000\n",
       " watch         0.003315\n",
       " water         0.000000\n",
       " wave          0.007330\n",
       " way           0.002923\n",
       " weak          0.000000\n",
       " white         0.005908\n",
       " whoa          0.009527\n",
       " wide          0.000000\n",
       " wild          0.000000\n",
       " win           0.000000\n",
       " wish          0.000000\n",
       " witness       0.000000\n",
       " woman         0.005679\n",
       " wonder        0.000000\n",
       " woo           0.004668\n",
       " word          0.003164\n",
       " work          0.003647\n",
       " world         0.000000\n",
       " worry         0.006761\n",
       " wrong         0.001744\n",
       " ya            0.000000\n",
       " yeah          0.008512\n",
       " year          0.000000\n",
       " yes           0.012717\n",
       " yesterday     0.000000\n",
       " yo            0.000000\n",
       " young         0.000000]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection_roc_aucs\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "random_forest_top_features = importances[3][0].sort_values(ascending = False)[:50]\n",
    "xgb_top_features = importances[5][0].sort_values(ascending = False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know       0.018959\n",
       "yeah       0.012899\n",
       "want       0.012732\n",
       "love       0.011587\n",
       "come       0.011437\n",
       "oh         0.011412\n",
       "like       0.009961\n",
       "feel       0.009281\n",
       "hear       0.008971\n",
       "let        0.008537\n",
       "thing      0.008474\n",
       "baby       0.008308\n",
       "day        0.008283\n",
       "right      0.008229\n",
       "time       0.008147\n",
       "need       0.008093\n",
       "lose       0.007978\n",
       "say        0.007851\n",
       "think      0.007795\n",
       "look       0.007690\n",
       "tell       0.007649\n",
       "away       0.007598\n",
       "way        0.007109\n",
       "hey        0.006971\n",
       "hand       0.006713\n",
       "light      0.006707\n",
       "night      0.006460\n",
       "long       0.006427\n",
       "man        0.006370\n",
       "eye        0.006341\n",
       "throw      0.006247\n",
       "girl       0.006175\n",
       "face       0.006098\n",
       "live       0.006054\n",
       "start      0.005881\n",
       "leave      0.005861\n",
       "home       0.005834\n",
       "mind       0.005726\n",
       "cause      0.005723\n",
       "dream      0.005719\n",
       "try        0.005667\n",
       "true       0.005600\n",
       "good       0.005573\n",
       "walk       0.005356\n",
       "turn       0.005258\n",
       "stay       0.005191\n",
       "boy        0.005189\n",
       "heart      0.005182\n",
       "tonight    0.005176\n",
       "whoa       0.005137\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "town        0.016324\n",
       "money       0.012873\n",
       "yes         0.012717\n",
       "hear        0.012581\n",
       "right       0.011584\n",
       "arm         0.011488\n",
       "damn        0.011459\n",
       "kick        0.011148\n",
       "big         0.010778\n",
       "hate        0.010303\n",
       "scream      0.010198\n",
       "room        0.010052\n",
       "run         0.010035\n",
       "walk        0.009701\n",
       "whoa        0.009527\n",
       "hey         0.009510\n",
       "rock        0.009500\n",
       "music       0.009449\n",
       "away        0.009367\n",
       "soon        0.009252\n",
       "home        0.009207\n",
       "hide        0.009176\n",
       "sweet       0.009169\n",
       "feeling     0.009120\n",
       "boy         0.009044\n",
       "ride        0.008982\n",
       "remember    0.008881\n",
       "nice        0.008874\n",
       "happy       0.008803\n",
       "til         0.008636\n",
       "yeah        0.008512\n",
       "come        0.008330\n",
       "leave       0.008306\n",
       "late        0.008288\n",
       "let         0.008269\n",
       "man         0.008242\n",
       "say         0.008121\n",
       "old         0.008057\n",
       "free        0.008046\n",
       "good        0.007717\n",
       "black       0.007675\n",
       "sit         0.007651\n",
       "lie         0.007615\n",
       "longer      0.007560\n",
       "sky         0.007490\n",
       "true        0.007417\n",
       "die         0.007343\n",
       "wave        0.007330\n",
       "make        0.007322\n",
       "wake        0.007218\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_top_features\n",
    "xgb_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'away',\n",
       " 'boy',\n",
       " 'come',\n",
       " 'good',\n",
       " 'hear',\n",
       " 'hey',\n",
       " 'home',\n",
       " 'leave',\n",
       " 'let',\n",
       " 'man',\n",
       " 'right',\n",
       " 'say',\n",
       " 'true',\n",
       " 'walk',\n",
       " 'whoa',\n",
       " 'yeah'}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature_overlap = set(random_forest_top_features.index) & set(xgb_top_features.index)\n",
    "\n",
    "top_feature_overlap\n",
    "\n",
    "len(top_feature_overlap)/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is a 32% overlap in the top 50 features for random forest and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X_train.to_csv('data/vectorized_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### taking the top 2 performing models and optimizing hyperparameters for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(model_hyperparam_dict, X_train, y_train):\n",
    "    '''\n",
    "    this function finds the best model.\n",
    "    takes in a dictionary with multiple algorithms and hyperparameters\n",
    "    returns the best model based on roc_auc cross val scores\n",
    "    also leaves a file with best params\n",
    "    '''\n",
    "    \n",
    "    keys = [k for k, v in model_hyperparam_dict.items()]\n",
    "    \n",
    "    cv_results = []\n",
    "    best_scores = []\n",
    "    best_estimators = []\n",
    "    classification_reports = []\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        model = model_hyperparam_dict[key]['model']\n",
    "        hyperparams = model_hyperparam_dict[key]['hyperparams']\n",
    "        \n",
    "        print(f'randomsearching {key}...')\n",
    "        print(f'using hyperparams: \\n{hyperparams}')\n",
    "        \n",
    "        grid = RandomizedSearchCV(\n",
    "            estimator = model,\n",
    "            param_distributions = hyperparams,\n",
    "            n_iter = 100,\n",
    "            scoring = 'roc_auc',\n",
    "            n_jobs = -1,\n",
    "            cv = StratifiedKFold(10, random_state = 42),\n",
    "            verbose = 2\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        cv_results.append(grid.cv_results_)\n",
    "        best_scores.append(grid.best_score_)\n",
    "        best_estimators.append(grid.best_estimator_)\n",
    "        \n",
    "    return cv_results, best_scores, best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparams = {\n",
    "    'bootstrap':[True, False],\n",
    "    'max_depth':[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features':['auto', 'sqrt'],\n",
    "    'min_samples_leaf':[1, 2, 4],\n",
    "    'min_samples_split':[2, 5, 10],\n",
    "    'n_estimators':[100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "xgb_hyperparams = {\n",
    "    'max_depth':range(1, 15),\n",
    "    'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators':[100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "    'gamma':[0, .01, .1, 1],\n",
    "    'min_child_weight':[1, 3, 5, 7],\n",
    "    'colsample_bytree':[.3, .4, .5, .6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparam_dict = {\n",
    "    'random_forest' : {\n",
    "        'model': RandomForestClassifier(random_state = 42),\n",
    "        'hyperparams' : random_forest_hyperparams\n",
    "    },\n",
    "    \n",
    "    'xgboost' : {\n",
    "        'model':XGBClassifier(random_state = 42),\n",
    "        'hyperparams' : xgb_hyperparams\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomsearching random_forest...\n",
      "using hyperparams: \n",
      "{'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 21.2min\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 68.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 107.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomsearching xgboost...\n",
      "using hyperparams: \n",
      "{'max_depth': range(1, 15), 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'gamma': [0, 0.01, 0.1, 1], 'min_child_weight': [1, 3, 5, 7], 'colsample_bytree': [0.3, 0.4, 0.5, 0.6]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/lucid75/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 47.2min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 121.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 210.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 345.9min finished\n"
     ]
    }
   ],
   "source": [
    "cv_results, best_scores, best_estimators = find_best_model(model_hyperparam_dict, vectorized_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5928416427745494, 0.584827035044296]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=10,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=1200,\n",
       "                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                        warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.4, gamma=0,\n",
       "               learning_rate=0.001, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=None, n_estimators=1200, n_jobs=1,\n",
       "               nthread=None, objective='binary:logistic', random_state=42,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "               silent=None, subsample=1, verbosity=1)]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = best_estimators[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = best_estimators[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pipeline_steps = [\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('variance_filter', variance_filter), # removes low variance columns from dataset\n",
    "    ('classifier', best_random_forest)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_pipeline_steps = [\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('variance_filter', variance_filter), # removes low variance columns from dataset\n",
    "    ('classifier', best_xgb)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_random_forest_model = Pipeline(second_pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=2,\n",
       "                                        min_samples_split=10,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=1200, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf_vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False...\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=0.4, gamma=0,\n",
       "                               learning_rate=0.001, max_delta_step=0,\n",
       "                               max_depth=13, min_child_weight=1, missing=None,\n",
       "                               n_estimators=1200, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=42,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_xgb_model = Pipeline(third_pipeline_steps)\n",
    "final_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_preds = final_random_forest_model.predict(X_test)\n",
    "random_forest_probas = final_random_forest_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = final_xgb_model.predict(X_test)\n",
    "xgb_probas = final_xgb_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536724961720811"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, random_forest_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.63      0.59       342\n",
      "         1.0       0.53      0.44      0.48       317\n",
      "\n",
      "    accuracy                           0.54       659\n",
      "   macro avg       0.54      0.54      0.53       659\n",
      "weighted avg       0.54      0.54      0.54       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, random_forest_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.129, 0.167]      2\n",
       "(0.167, 0.203]      1\n",
       "(0.203, 0.239]      1\n",
       "(0.239, 0.275]      5\n",
       "(0.275, 0.311]      8\n",
       "(0.311, 0.347]     18\n",
       "(0.347, 0.383]     44\n",
       "(0.383, 0.419]     66\n",
       "(0.419, 0.455]     76\n",
       "(0.455, 0.491]    111\n",
       "(0.491, 0.527]     89\n",
       "(0.527, 0.563]     63\n",
       "(0.563, 0.599]     43\n",
       "(0.599, 0.635]     34\n",
       "(0.635, 0.671]     46\n",
       "(0.671, 0.707]     25\n",
       "(0.707, 0.743]     12\n",
       "(0.743, 0.779]      5\n",
       "(0.779, 0.815]      7\n",
       "(0.815, 0.851]      3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([l[1] for l in probas]).value_counts(bins = 20).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5618047484642205"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.55      0.57       342\n",
      "         1.0       0.54      0.57      0.56       317\n",
      "\n",
      "    accuracy                           0.56       659\n",
      "   macro avg       0.56      0.56      0.56       659\n",
      "weighted avg       0.56      0.56      0.56       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, xgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.291, 0.311]     1\n",
       "(0.311, 0.329]     3\n",
       "(0.329, 0.347]     1\n",
       "(0.347, 0.366]     5\n",
       "(0.366, 0.384]    14\n",
       "(0.384, 0.402]    14\n",
       "(0.402, 0.42]     24\n",
       "(0.42, 0.439]     31\n",
       "(0.439, 0.457]    52\n",
       "(0.457, 0.475]    66\n",
       "(0.475, 0.493]    90\n",
       "(0.493, 0.512]    62\n",
       "(0.512, 0.53]     86\n",
       "(0.53, 0.548]     51\n",
       "(0.548, 0.566]    36\n",
       "(0.566, 0.585]    88\n",
       "(0.585, 0.603]    16\n",
       "(0.603, 0.621]    12\n",
       "(0.621, 0.639]     3\n",
       "(0.639, 0.658]     4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([l[1] for l in xgb_probas]).value_counts(bins = 20).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_xgb_model, open('xgb_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"xgb_model.tar.gz\", \"w:gz\")\n",
    "tar.add('xgb_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_recommendations = word_transformer.transform(null_df['cleaned_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38071, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['recommendation'] = [l[1] for l in nb.predict_proba(vectorized_recommendations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.to_csv('first_recommendations.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>liked</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>playlist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI can't remember\\nThe words wer...</td>\n",
       "      <td>If You Want It</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remember word form mouth have find bring joy p...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI'm not that kind of fool\\nWho ...</td>\n",
       "      <td>Superbike</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kind fool need read room somebody tell fall li...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\n[Verse 1]\\nPoint me to my chair\\nMake me s...</td>\n",
       "      <td>Peace Out</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>point chair sing awful song bear go hard hard ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\n[Verse 1]\\nUsed to be the one to cry\\nAnd ...</td>\n",
       "      <td>Devotion</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cry feel emotion need path find strange devoti...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\n[Verse 1]\\nI'm sinking in my bed\\nWe’re le...</td>\n",
       "      <td>Nighttime Drive</td>\n",
       "      <td>Jay Som</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sink bed be leave town tomorrow memory feel nu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics       song_title  \\\n",
       "3  \\n\\n[Verse 1]\\nI can't remember\\nThe words wer...   If You Want It   \n",
       "4  \\n\\n[Verse 1]\\nI'm not that kind of fool\\nWho ...        Superbike   \n",
       "5  \\n\\n[Verse 1]\\nPoint me to my chair\\nMake me s...        Peace Out   \n",
       "6  \\n\\n[Verse 1]\\nUsed to be the one to cry\\nAnd ...         Devotion   \n",
       "7  \\n\\n[Verse 1]\\nI'm sinking in my bed\\nWe’re le...  Nighttime Drive   \n",
       "\n",
       "  artist_name  liked                                     cleaned_lyrics  \\\n",
       "3     Jay Som    NaN  remember word form mouth have find bring joy p...   \n",
       "4     Jay Som    NaN  kind fool need read room somebody tell fall li...   \n",
       "5     Jay Som    NaN  point chair sing awful song bear go hard hard ...   \n",
       "6     Jay Som    NaN  cry feel emotion need path find strange devoti...   \n",
       "7     Jay Som    NaN  sink bed be leave town tomorrow memory feel nu...   \n",
       "\n",
       "   playlist  \n",
       "3       1.0  \n",
       "4       1.0  \n",
       "5       1.0  \n",
       "6       1.0  \n",
       "7       0.0  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df_count_vect = count_vect_no_stops.transform(null_df['cleaned_lyrics'])\n",
    "null_df_tfidf = tfidf_transformer.transform(null_df_count_vect)\n",
    "music_to_listen = sgd_ns.predict_proba(null_df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['playlist'] = music_to_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21744"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16327"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(null_df[null_df['playlist'] > .5])\n",
    "len(null_df[null_df['playlist'] < .5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
